{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CieBoKNE7u-D"
   },
   "source": [
    "**Passos para criar um perceptron usando TensorFlow:**\n",
    "*   Importar as bibliotecas necessárias\n",
    "*   Carregar e preparar o dataset\n",
    "*   Definir e compilar o modelo do perceptron\n",
    "*   Treinar o modelo\n",
    "*   Avaliar o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebqvaVC-6zMQ"
   },
   "source": [
    "Importando as bibliotecas e carregando o dataset: Importamos o TensorFlow para criar o modelo e funções do scikit-learn para manipular o dataset e realizar a padronização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PplC7Shu05xP"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        x1      x2       x3  d\n",
      "0  -0.6508  0.1097   4.0009 -1\n",
      "1  -1.4492  0.8896   4.4005 -1\n",
      "2   2.0850  0.6876  12.0710 -1\n",
      "3   0.2626  1.1476   7.7985  1\n",
      "4   0.6418  1.0234   7.0427  1\n",
      "5   0.2569  0.6730   8.3265 -1\n",
      "6   1.1155  0.6043   7.4446  1\n",
      "7   0.0914  0.3399   7.0677 -1\n",
      "8   0.0121  0.5256   4.6316  1\n",
      "9  -0.0429  0.4660   5.4323  1\n",
      "10  0.4340  0.6870   8.2287 -1\n",
      "11  0.2735  1.0287   7.1934  1\n",
      "12  0.4839  0.4851   7.4850 -1\n",
      "13  0.4089 -0.1267   5.5019 -1\n",
      "14  1.4391  0.1614   8.5843 -1\n",
      "15 -0.9115 -0.1973   2.1962 -1\n",
      "16  0.3654  1.0475   7.4858  1\n",
      "17  0.2144  0.7515   7.1699  1\n",
      "18  0.2013  1.0014   6.5489  1\n",
      "19  0.6483  0.2183   5.8991  1\n",
      "20 -0.1147  0.2242   7.2435 -1\n",
      "21 -0.7970  0.8795   3.8762  1\n",
      "22 -1.0625  0.6366   2.4707  1\n",
      "23  0.5307  0.1285   5.6883  1\n",
      "24 -1.2200  0.7777   1.7252  1\n",
      "25  0.3957  0.1076   5.6623 -1\n",
      "26 -0.1013  0.5989   7.1812 -1\n",
      "27  2.4482  0.9455  11.2095  1\n",
      "28  2.0149  0.6192  10.9263 -1\n",
      "29  0.2012  0.2611   5.4631  1\n"
     ]
    }
   ],
   "source": [
    "# Carregar o arquivo XLS com os dados\n",
    "file_path = 'Tabela_Perceptron.xls'\n",
    "data_xls = pd.read_excel(file_path)\n",
    "\n",
    "# Exibir os dados para verificação\n",
    "print(data_xls.to_string())\n",
    "\n",
    "# Separar os dados em features (X) e labels (y)\n",
    "X = data_xls[['x1', 'x2', 'x3']].values\n",
    "y = data_xls['d'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MeCdk8266dqd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 11:38:49.664490: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-08-13 11:38:49.664842: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 11:38:49.667083: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Criar o modelo de rede neural\n",
    "net = Sequential()\n",
    "# Usando Flatten para transformar a saída das camadas convolucionais em um vetor\n",
    "net.add(Flatten())\n",
    "net.add(Dense(256, activation='relu'))\n",
    "net.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter valores específicos para 0\n",
    "y = np.where(y == -1, 0, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjeM_aehY2q9",
    "outputId": "6598f8f3-287b-49c8-af39-d75888546e9c"
   },
   "outputs": [],
   "source": [
    "# Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados para o intervalo [0, 1]\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "0lSwTDhWX4bI",
    "outputId": "69e72eba-cac8-4434-ee9a-d7f5668c9ca2"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Resumo do modelo\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/RNA/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:2376\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   2359\u001b[0m \n\u001b[1;32m   2360\u001b[0m \u001b[38;5;124;03mArguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2373\u001b[0m \u001b[38;5;124;03m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   2374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[0;32m-> 2376\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis model has not yet been built. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2377\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuild the model first by calling `build()` or calling \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2378\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`fit()` with some data, or specify \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2379\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124man `input_shape` argument in the first layer(s) for \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2380\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautomatic build.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2381\u001b[0m layer_utils\u001b[38;5;241m.\u001b[39mprint_summary(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2382\u001b[0m                           line_length\u001b[38;5;241m=\u001b[39mline_length,\n\u001b[1;32m   2383\u001b[0m                           positions\u001b[38;5;241m=\u001b[39mpositions,\n\u001b[1;32m   2384\u001b[0m                           print_fn\u001b[38;5;241m=\u001b[39mprint_fn)\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "# Resumo do modelo\n",
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYedLz2ZZBtP"
   },
   "outputs": [],
   "source": [
    "# Compilar o modelo\n",
    "net.compile(tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uIP2aa5TV4VC",
    "outputId": "3d01acd2-bc17-4991-e9e9-cdc1235176c3"
   },
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "history = net.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WafG6OwNaLFE",
    "outputId": "4e8abab6-ca13-4627-9a9e-62c049c5baa2"
   },
   "outputs": [],
   "source": [
    "# Avaliar o modelo\n",
    "test_loss, test_acc = net.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "YY8mSUdgaWK3",
    "outputId": "1a695115-3858-4d22-9352-c0240696f361"
   },
   "outputs": [],
   "source": [
    "# Plotar as curvas de perda e acurácia\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Perda durante o treinamento e validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.title('Acurácia durante o treinamento e validação')\n",
    "plt.xlabel('Épocas')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmyAXKALT5Df"
   },
   "outputs": [],
   "source": [
    "# Salvar o modelo treinado\n",
    "net.save(\"meu_fashion_mnist_cnn_model_sgd.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZgmeISe263f"
   },
   "source": [
    "Exercícios\n",
    "¶\n",
    "1) Altere o valor do hiperparâmetro num_hiddens e veja como esse hiperparâmetro influencia seus resultados. Determine o melhor valor deste hiperparâmetro, mantendo todos os outros constantes.\n",
    "\n",
    "2) Experimente adicionar uma camada oculta adicional para ver como isso afeta os resultados.\n",
    "\n",
    "3) Como mudar a taxa de aprendizado altera seus resultados? Corrigindo a arquitetura do modelo e outros hiperparâmetros (incluindo o número de épocas), qual taxa de aprendizado oferece os melhores resultados?\n",
    "\n",
    "4) Qual é o melhor resultado que você pode obter otimizando todos os hiperparâmetros (taxa de aprendizagem, número de épocas, número de camadas ocultas, número de unidades ocultas por camada) em conjunto?\n",
    "\n",
    "5) Descreva por que é muito mais difícil lidar com vários hiperparâmetros.\n",
    "\n",
    "6) Qual é a estratégia mais inteligente que você pode imaginar para estruturar uma pesquisa em vários hiperparâmetros?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
