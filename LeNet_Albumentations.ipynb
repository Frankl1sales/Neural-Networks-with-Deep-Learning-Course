{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3N07AdcZWzIqGU3OMQTH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Frankl1sales/Neural-Networks-with-Deep-Learning-Course/blob/main/LeNet_Albumentations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT_LqsL9xThN",
        "outputId": "2ba0facc-fd0b-4d28-bd8a-5467d4d912c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.14)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: scikit-image>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.23.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.12.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.8.2)\n",
            "Requirement already satisfied: albucore>=0.0.13 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.13)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from albucore>=0.0.13->albumentations) (2.0.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.20.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (9.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (2024.8.10)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Activation, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "import csv\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "print(train_images.shape)\n",
        "print(test_images.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)\n",
        "\n",
        "# Guardar uma cópia dos labels originais\n",
        "ori_test_labels = copy.deepcopy(test_labels)\n",
        "\n",
        "# One-hot encoding dos labels\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# Redimensionar as imagens para incluir o canal de cores (1 canal para grayscale)\n",
        "img_rows, img_cols = train_images.shape[1], train_images.shape[2]\n",
        "train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n",
        "test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n",
        "print('train_images shape:', train_images.shape)\n",
        "print('test_images shape:', test_images.shape)\n",
        "print(train_images.shape[0], 'train samples')\n",
        "print(test_images.shape[0], 'test samples')\n",
        "\n",
        "# Normalizar as imagens para terem média zero e desvio unitário\n",
        "train_mean = train_images.mean()\n",
        "train_std = train_images.std()\n",
        "\n",
        "train_images = (train_images - train_mean) / train_std\n",
        "test_images = (test_images - train_mean) / train_std\n",
        "\n",
        "# Definir as transformações de augmentação usando albumentations\n",
        "augmentations = A.Compose([\n",
        "    A.Rotate(limit=10, p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "\n",
        "# Function to apply augmentation\n",
        "def augment_data(images, labels):\n",
        "    augmenter = ImageDataGenerator(\n",
        "        rotation_range=5,  # Rotating randomly the images up to 10°\n",
        "        width_shift_range=0.05,  # Moving the images from left to right\n",
        "        height_shift_range=0.05,  # Then from top to bottom\n",
        "        shear_range=0.10,\n",
        "        zoom_range=0.05,  # Zooming randomly up to 5%\n",
        "        zca_whitening=False,\n",
        "        horizontal_flip=False,\n",
        "        vertical_flip=False,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Convert the images to the correct data type\n",
        "    images = images.astype(np.float32)\n",
        "\n",
        "    # Apply augmentation\n",
        "    augmented_images = []\n",
        "    augmented_labels = []\n",
        "    for i in range(len(images)):\n",
        "        img = images[i]\n",
        "        label = labels[i]\n",
        "        img = augmenter.random_transform(img)\n",
        "        augmented_images.append(img)\n",
        "        augmented_labels.append(label)\n",
        "\n",
        "    return np.array(augmented_images), np.array(augmented_labels)\n",
        "\n",
        "# Apply the augmentation to the training set\n",
        "train_images, train_labels = augment_data(train_images, train_labels)\n",
        "\n",
        "# Normalize the augmented images (if necessary)\n",
        "train_images = (train_images - train_mean) / train_std\n",
        "\n",
        "# Definir a arquitetura do modelo\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "pool_size = (2, 2)\n",
        "kernel_size = (5, 5)\n",
        "num_kernel_first_conv_layer = 6\n",
        "num_kernel_second_conv_layer = 16\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(num_kernel_first_conv_layer, kernel_size, input_shape=input_shape, padding='same'))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Conv2D(num_kernel_second_conv_layer, kernel_size, padding='same'))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(MaxPooling2D(pool_size=pool_size))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100))\n",
        "model.add(Activation('sigmoid'))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "# Definir o otimizador\n",
        "optimizer = tf.optimizers.SGD(learning_rate=1e-2)\n",
        "\n",
        "# Compilar o modelo\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Treinar o modelo\n",
        "BATCH_SIZE = 65\n",
        "EPOCHS = 5\n",
        "batch_history = model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(test_images, test_labels))\n",
        "\n",
        "# Avaliar o modelo\n",
        "result = model.evaluate(test_images, test_labels)\n",
        "print(f'Test accuracy: {result[1]}')\n",
        "\n",
        "# Plotar as curvas de perda e acurácia\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
        "\n",
        "# Perda\n",
        "axs[0].plot(batch_history.history['loss'], label='Perda de Treinamento')\n",
        "axs[0].plot(batch_history.history['val_loss'], label='Perda de Validação')\n",
        "axs[0].set_title('Função de Perda/ Model Loss')\n",
        "axs[0].set_xlabel('Épocas/Epoch')\n",
        "axs[0].set_ylabel('Perda/Loss')\n",
        "axs[0].legend(['train', 'test'], loc='upper right')\n",
        "\n",
        "# Acurácia\n",
        "axs[1].plot(batch_history.history['accuracy'], label='Acurácia de Treinamento')\n",
        "axs[1].plot(batch_history.history['val_accuracy'], label='Acurácia de Validação')\n",
        "axs[1].set_title('Acurácia/Accuracy')\n",
        "axs[1].set_xlabel('Épocas/Epoch')\n",
        "axs[1].set_ylabel('Acurácia/Accuracy')\n",
        "axs[1].legend(['train', 'test'], loc='lower right')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Matriz de confusão\n",
        "predictions = model.predict(test_images)\n",
        "matrix = confusion_matrix(test_labels.argmax(axis=1), predictions.argmax(axis=1))\n",
        "print(matrix)\n",
        "\n",
        "# Salvar o modelo e informações adicionais\n",
        "save_dir = os.path.join(os.getcwd(), 'trained_lenet5_mnist')\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "model_name = 'model.keras'\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)\n",
        "\n",
        "# Salvar média e desvio padrão como CSV\n",
        "model_name = 'std_dev.csv'\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "w = csv.writer(open(model_path, \"w\"))\n",
        "dict = {'mean': train_mean, 'std': train_std}\n",
        "for key, val in dict.items():\n",
        "    w.writerow([key, val])\n",
        "print('Saved side information at %s ' % model_path)\n",
        "\n",
        "# Plotar a distribuição dos labels originais\n",
        "plt.figure(figsize=(10,5))\n",
        "datas = pd.DataFrame(ori_test_labels, columns=['label'])\n",
        "sns.countplot(x='label', data=datas)\n",
        "plt.title('Distribution of labels in training set')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqzIZOGu0jgF",
        "outputId": "0c61aac3-5c70-4574-d1db-55ee844b55a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000,)\n",
            "(10000,)\n",
            "train_images shape: (60000, 28, 28, 1)\n",
            "test_images shape: (10000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "Epoch 1/5\n",
            "\u001b[1m924/924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 51ms/step - accuracy: 0.1042 - loss: 2.3280 - val_accuracy: 0.1009 - val_loss: 2.3043\n",
            "Epoch 2/5\n",
            "\u001b[1m609/924\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 49ms/step - accuracy: 0.1066 - loss: 2.3027"
          ]
        }
      ]
    }
  ]
}