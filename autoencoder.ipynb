{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# O que é Autoencoder?\nUm autoencoder é uma rede neural que aprende a codificar os dados em uma representação comprimida (espaço latente) e depois tenta reconstruí-los o mais próximo possível dos dados originais.\n\nFontes: \n1. [Introdução aos codificadores automáticos](https://www.tensorflow.org/tutorials/generative/autoencoder?hl=pt-br)\n2. [Cap 14 - Autoencoders](https://www.deeplearningbook.org/)\n3. [IBM -What is an autoencoder?](https://www.ibm.com/topics/autoencoder)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.451973Z","iopub.execute_input":"2024-10-09T17:45:42.452416Z","iopub.status.idle":"2024-10-09T17:45:42.458677Z","shell.execute_reply.started":"2024-10-09T17:45:42.452373Z","shell.execute_reply":"2024-10-09T17:45:42.457515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Carregamento do dataset \nO dataset **Fashion MNIST** é carregado, ele possui imagens de roupas e acessórios, com 60.0000 imagens para treino e 10.000 para testes.\n* As variáveis x_train e x_test contêm essas respectivas imagens\n* O uso de _ ignora os rótulos (labels)","metadata":{}},{"cell_type":"code","source":"(x_train, _), (x_test, _) = fashion_mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.461481Z","iopub.execute_input":"2024-10-09T17:45:42.462415Z","iopub.status.idle":"2024-10-09T17:45:42.839230Z","shell.execute_reply.started":"2024-10-09T17:45:42.462360Z","shell.execute_reply":"2024-10-09T17:45:42.838303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As imagens são originalmente representadas como arrays de inteiros de 8 bits (valores entre 0 e 255), e são convertidas para float32 (ponto flutuante de 32 bits).\n* Elas são normalizadas dividindo cada valor de pixel por 255, para que os valores fiquem no intervalo de [0,1].\n* A normalização facilita o treinamento de modelos de aprendizado de máquina","metadata":{}},{"cell_type":"code","source":"x_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.840485Z","iopub.execute_input":"2024-10-09T17:45:42.840885Z","iopub.status.idle":"2024-10-09T17:45:42.919785Z","shell.execute_reply.started":"2024-10-09T17:45:42.840845Z","shell.execute_reply":"2024-10-09T17:45:42.918582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Implementa o autoencoder\nO código a seguir implementa o autoencoder utilizando a API **Model** do TensorFlow/Keras. \n* latent_dim: define o tamanho de espaço latente = número de unidades (neurônios) na camada de codificação comprimida. No caso 64.\n* shape: a forma original dos dados de entrada [imagens]. No caso, (28,28)","metadata":{}},{"cell_type":"code","source":"class Autoencoder(Model):\n  def __init__(self, latent_dim, shape):\n    super(Autoencoder, self).__init__()\n    self.latent_dim = latent_dim\n    self.shape = shape\n    self.encoder = tf.keras.Sequential([\n      layers.Flatten(),\n      layers.Dense(latent_dim, activation='relu'),\n    ])\n    self.decoder = tf.keras.Sequential([\n      layers.Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'),\n      layers.Reshape(shape)\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.921197Z","iopub.execute_input":"2024-10-09T17:45:42.921583Z","iopub.status.idle":"2024-10-09T17:45:42.928921Z","shell.execute_reply.started":"2024-10-09T17:45:42.921530Z","shell.execute_reply":"2024-10-09T17:45:42.927808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder\nO encoder é responsável por comprimir a entrada para o espaço latente.\n* Flatten(): esta camada transforma as imagens 2D (de forma (28,28)) em vetores 1D de 784 (28*28) elementos.\n* Dense(latent_dim, activation='relu'): após o flattening, os vetores são passados por uma camada densa com latent_dim unidades e ativação ReLU. -- Isso comprime as informações para um vetor de tamanho latent_dim (neste caso, 64 dimensões) ","metadata":{}},{"cell_type":"markdown","source":"![dense layer 2](https://pysource.com/wp-content/uploads/2022/08/flatten-and-dense-layers-computer-vision-with-keras-p-6-dense-layer-scheme-1024x723.jpg)\nFonte: https://pysource.com/2022/10/07/flatten-and-dense-layers-computer-vision-with-keras-p-6/","metadata":{}},{"cell_type":"markdown","source":"![dense layer](https://epynn.net/_images/Dense-01.svg)\nFonte: https://epynn.net/Dense.html","metadata":{}},{"cell_type":"markdown","source":"# Decoder\nO decoder é responsável por reconstruir a imagem a partir da representação comprimida (espaço latente).\n* Dense(tf.math.reduce_prod(shape).numpy(), activation='sigmoid'): A primeira camada densa reconstrói o veotr 1D, que tem o mesmo número de elementos que a imagem original (784), utilizando a função de ativação sigmoid -- isso se justifica pelo fato que os valores dos pixels foram normalizados entre 0 e 1.\n* Reshape(shape): transforma o vetor 1D de volta para a forma original da imagem (28, 28)","metadata":{}},{"cell_type":"markdown","source":"# Método call\nMétodo call define a passagem dos dados pelos autoencoder\n* encoded:  a entrada é primeiro passada pelo encoder, onde é comprimida para o espaço latente.\n* decoded: a representação comprimida é passada pelo decoder, que tenta reconstruir a imagem original.\n* o método retorna a imagem reconstruída.","metadata":{}},{"cell_type":"code","source":"shape = x_test.shape[1:]\nlatent_dim = 64\nautoencoder = Autoencoder(latent_dim, shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.932197Z","iopub.execute_input":"2024-10-09T17:45:42.932952Z","iopub.status.idle":"2024-10-09T17:45:42.947729Z","shell.execute_reply.started":"2024-10-09T17:45:42.932908Z","shell.execute_reply":"2024-10-09T17:45:42.946727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inicialização do Autoencoder\n* latent_dim = 64: O número de neurônios no espaço latente.\n* shape = x_test.shape[1:]: A forma original das imagens de entrada (no caso do Fashion MNIST, (28, 28)).","metadata":{}},{"cell_type":"markdown","source":"# Resumo:\nEste autoencoder comprime uma imagem de 28x28 pixels em um vetor de 64 dimensões (espaço latente) através do encoder e depois tenta reconstruir a imagem original a partir dessa representação comprimida através do decoder. A rede é projetada para treinar de forma que a saída seja o mais semelhante possível à entrada, forçando o autoencoder a aprender uma representação eficiente dos dados.","metadata":{}},{"cell_type":"markdown","source":"Compile(): é usado para configurar o modelo antes do treinamento\n* otimizador: como os pesos do modelo serão atualizados durante o treinamento\n* função de perda: a métrica que será usada para avaliar o quão bom o modelo está se saindo, ou seja, quão bem as saídas (imagens reconstruídas) se aproximam dos dados reais (imagens de entrada).","metadata":{}},{"cell_type":"code","source":"autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.949740Z","iopub.execute_input":"2024-10-09T17:45:42.950392Z","iopub.status.idle":"2024-10-09T17:45:42.961903Z","shell.execute_reply.started":"2024-10-09T17:45:42.950322Z","shell.execute_reply":"2024-10-09T17:45:42.960460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"optmizer = 'adam'\nAdam (Adaptive Moment Estimation): combina as vantagens de dois outros algoritmos AdaGrad e RMSProp\n\nloss=losses.MeanSquaredError()\nfunção de perda\n![mse](https://suboptimal.wiki/images/mse_5.jpg)\nfonte: https://suboptimal.wiki/explanation/mse/\n* n é o número de exemplos no conjunto de dados.\n* yi​ é o valor real (neste caso, o pixel da imagem original).\n* y^​i​ é o valor previsto pelo modelo (o pixel da imagem reconstruída).","metadata":{}},{"cell_type":"markdown","source":"A seguir é realizado o treinamento do modelo autoencoder, ajustando os pesos com base nos dados de treinamento. \n* autoencoder.fit(): método fit() em Keras é usado para treinar o modelo em um conjunto de dados de entrada.\n*  shuffle=True: indica que o conjunto de dados será embaralhado antes de cada época. Isso evita que o modelo aprenda padrões indesejados ou artefatos que possam existir na sequência original dos dados","metadata":{}},{"cell_type":"code","source":"autoencoder.fit(x_train, x_train,\n                epochs=10,\n                shuffle=True,\n                validation_data=(x_test, x_test))","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:45:42.963342Z","iopub.execute_input":"2024-10-09T17:45:42.963790Z","iopub.status.idle":"2024-10-09T17:46:32.898612Z","shell.execute_reply.started":"2024-10-09T17:45:42.963736Z","shell.execute_reply":"2024-10-09T17:46:32.897601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"validation_data=(x_test, x_test)\n* Durante o treinamento, além de ajustar os pesos com base nos dados de treinamento, é útil avaliar o desempenho do modelo em dados que não foram usados no treinamento para ver como ele se generaliza.","metadata":{}},{"cell_type":"markdown","source":"# Utilização do modelo autoencoder\nautoencoder.encoder(x_test): Aqui, estamos utilizando a parte encoder do autoencoder, que é responsável por transformar (ou codificar) as imagens de entrada em uma representação compacta ou espaço latente.\n* O x_test (imagens de teste) é passado para o encoder, que aplica a função Dense(latent_dim, activation='relu'), resultando em uma versão codificada ou comprimida das imagens.\n* Essa codificação contém menos dimensões do que a imagem original e retém as informações mais importantes, ou seja, é a versão \"compactada\" das imagens originais.","metadata":{}},{"cell_type":"markdown","source":"* numpy(): Depois de passar as imagens pelo encoder, o resultado está em formato de tensor do TensorFlow. O método .numpy() converte esse tensor para um array do NumPy, que é um formato comum para manipulação de dados em Python.\n* encoded_imgs: Este array contém as representações codificadas (ou \"comprimidas\") das imagens de teste. São representações no espaço latente, que têm um número reduzido de dimensões em comparação com as imagens originais.","metadata":{}},{"cell_type":"code","source":"encoded_imgs = autoencoder.encoder(x_test).numpy()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:32.899874Z","iopub.execute_input":"2024-10-09T17:46:32.900205Z","iopub.status.idle":"2024-10-09T17:46:32.926194Z","shell.execute_reply.started":"2024-10-09T17:46:32.900169Z","shell.execute_reply":"2024-10-09T17:46:32.924872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"autoencoder.decoder(encoded_imgs): Aqui, estamos passando as imagens codificadas (representações latentes) para a parte decoder do autoencoder, que é responsável por reconstruir as imagens originais a partir dessas representações compactadas.\n\n* O decoder aplica uma camada densa seguida de uma camada de Reshape para reconstruir as imagens a partir do espaço latente.\n* A saída do decoder deve ter o mesmo formato que as imagens originais (no caso do Fashion MNIST, uma imagem de 28x28 pixels).\n\n* numpy(): Assim como no encoder, o método .numpy() converte o tensor de saída do TensorFlow para um array do NumPy.\n\n* decoded_imgs: Este array contém as imagens reconstruídas a partir das representações latentes. Essas imagens são o resultado da tentativa do autoencoder de reconstruir as imagens de entrada a partir das informações comprimidas.","metadata":{}},{"cell_type":"code","source":"decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:32.927592Z","iopub.execute_input":"2024-10-09T17:46:32.927950Z","iopub.status.idle":"2024-10-09T17:46:32.963211Z","shell.execute_reply.started":"2024-10-09T17:46:32.927911Z","shell.execute_reply":"2024-10-09T17:46:32.962051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Comparação entre original e reconstruído\nEsse código cria uma visualização comparativa entre as imagens originais e as imagens reconstruídas geradas pelo autoencoder para 10 exemplos do conjunto de teste x_test","metadata":{}},{"cell_type":"code","source":"n = 10\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n  # display original\n  ax = plt.subplot(2, n, i + 1)\n  plt.imshow(x_test[i])\n  plt.title(\"original\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\n\n  # display reconstruction\n  ax = plt.subplot(2, n, i + 1 + n)\n  plt.imshow(decoded_imgs[i])\n  plt.title(\"reconstructed\")\n  plt.gray()\n  ax.get_xaxis().set_visible(False)\n  ax.get_yaxis().set_visible(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:32.964560Z","iopub.execute_input":"2024-10-09T17:46:32.965010Z","iopub.status.idle":"2024-10-09T17:46:34.031504Z","shell.execute_reply.started":"2024-10-09T17:46:32.964969Z","shell.execute_reply":"2024-10-09T17:46:34.030219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Aplicando ruído às imagens\n","metadata":{}},{"cell_type":"code","source":"(x_train, _), (x_test, _) = fashion_mnist.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:34.032961Z","iopub.execute_input":"2024-10-09T17:46:34.033316Z","iopub.status.idle":"2024-10-09T17:46:34.410518Z","shell.execute_reply.started":"2024-10-09T17:46:34.033277Z","shell.execute_reply":"2024-10-09T17:46:34.409585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:34.411883Z","iopub.execute_input":"2024-10-09T17:46:34.412231Z","iopub.status.idle":"2024-10-09T17:46:34.490916Z","shell.execute_reply.started":"2024-10-09T17:46:34.412193Z","shell.execute_reply":"2024-10-09T17:46:34.489777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Expansão da Dimensão: \n* **tf.newaxis**: Adiciona uma nova dimensão ao array. Este comando é usado para expandir a forma das imagens de (num_images, height, width) para (num_images, height, width, 1). Essa nova dimensão é frequentemente necessária para que o modelo de autoencoder trate as imagens como dados de entrada com formato (altura, largura, canais), onde o número de canais é 1 (para imagens em escala de cinza).","metadata":{}},{"cell_type":"code","source":"x_train = x_train[..., tf.newaxis]\nx_test = x_test[..., tf.newaxis]","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:34.492264Z","iopub.execute_input":"2024-10-09T17:46:34.492639Z","iopub.status.idle":"2024-10-09T17:46:34.497618Z","shell.execute_reply.started":"2024-10-09T17:46:34.492600Z","shell.execute_reply":"2024-10-09T17:46:34.496445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* noise_factor = 0.2: Define a intensidade do ruído a ser adicionado. Neste caso, um fator de 0.2 significa que o ruído adicionado terá um impacto considerável, mas não extremo, na imagem original","metadata":{}},{"cell_type":"markdown","source":"* tf.random.normal(shape=x_train.shape): Gera um tensor de números aleatórios com distribuição normal (ou gaussiana) com a mesma forma que x_train (ou x_test). O ruído gerado terá uma média de 0 e um desvio padrão de 1.\n* x_train + noise_factor * tf.random.normal(...): Adiciona o ruído normalizado às imagens originais. O ruído é multiplicado pelo noise_factor para controlar a quantidade de ruído a ser adicionado. Isso cria a versão \"ruidosa\" das imagens.","metadata":{}},{"cell_type":"code","source":"noise_factor = 0.2\nx_train_noisy = x_train + noise_factor * tf.random.normal(shape=x_train.shape)\nx_test_noisy = x_test + noise_factor * tf.random.normal(shape=x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:34.500879Z","iopub.execute_input":"2024-10-09T17:46:34.501241Z","iopub.status.idle":"2024-10-09T17:46:35.551362Z","shell.execute_reply.started":"2024-10-09T17:46:34.501203Z","shell.execute_reply":"2024-10-09T17:46:35.549972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tf.clip_by_value(...): Essa função é usada para garantir que todos os valores no tensor fiquem dentro de um intervalo específico. Aqui, ela assegura que todos os valores em x_train_noisy e x_test_noisy permaneçam entre 0 e 1. \n* Isso é importante porque, após adicionar ruído, alguns valores podem exceder 1 (o que não é válido para imagens normalizadas) ou ser menores que 0. Clipping impede que isso ocorra, mantendo os dados dentro da faixa esperada.","metadata":{}},{"cell_type":"code","source":"x_train_noisy = tf.clip_by_value(x_train_noisy, clip_value_min=0., clip_value_max=1.)\nx_test_noisy = tf.clip_by_value(x_test_noisy, clip_value_min=0., clip_value_max=1.)","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:35.552690Z","iopub.execute_input":"2024-10-09T17:46:35.553160Z","iopub.status.idle":"2024-10-09T17:46:35.688180Z","shell.execute_reply.started":"2024-10-09T17:46:35.553114Z","shell.execute_reply":"2024-10-09T17:46:35.686871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 10\nplt.figure(figsize=(20, 2))\nfor i in range(n):\n    ax = plt.subplot(1, n, i + 1)\n    plt.title(\"original + noise\")\n    plt.imshow(tf.squeeze(x_test_noisy[i]))\n    plt.gray()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:35.689639Z","iopub.execute_input":"2024-10-09T17:46:35.690072Z","iopub.status.idle":"2024-10-09T17:46:36.979198Z","shell.execute_reply.started":"2024-10-09T17:46:35.690026Z","shell.execute_reply":"2024-10-09T17:46:36.978090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Denoise(Model):\n  def __init__(self):\n    super(Denoise, self).__init__()\n    self.encoder = tf.keras.Sequential([\n      layers.Input(shape=(28, 28, 1)),\n      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n\n    self.decoder = tf.keras.Sequential([\n      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n      layers.Conv2D(1, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n\nautoencoder = Denoise()","metadata":{"execution":{"iopub.status.busy":"2024-10-09T17:46:36.980776Z","iopub.execute_input":"2024-10-09T17:46:36.981156Z","iopub.status.idle":"2024-10-09T17:46:37.018374Z","shell.execute_reply.started":"2024-10-09T17:46:36.981116Z","shell.execute_reply":"2024-10-09T17:46:37.017285Z"},"trusted":true},"execution_count":null,"outputs":[]}]}